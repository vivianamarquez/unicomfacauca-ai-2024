{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsKcJT8Un8Ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPfxNtvao8vF",
        "outputId": "c3a1c740-36e0-4c31-e101-4eece8ac9f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function to convert text into a usable form\n",
        "def preprocess_text(text):\n",
        "    chars = sorted(list(set(text)))  # Get all unique characters\n",
        "    char_to_idx = {ch: i for i, ch in enumerate(chars)}  # Char -> index\n",
        "    idx_to_char = {i: ch for i, ch in enumerate(chars)}  # Index -> char\n",
        "\n",
        "    return chars, char_to_idx, idx_to_char"
      ],
      "metadata": {
        "id": "T6UYfY1an-9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text to work with\n",
        "with open('obama.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "jhzgz_jloAox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "chars, char_to_idx, idx_to_char = preprocess_text(text)\n",
        "input_size = len(chars)  # Total unique characters in the text\n",
        "hidden_size = 128        # Size of the hidden layers\n",
        "seq_length = 100         # Length of the input sequence (can be adjusted)\n",
        "batch_size = 64          # Batch size\n",
        "learning_rate = 0.002    # Learning rate"
      ],
      "metadata": {
        "id": "E1Jmd7PnoB0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to a sequence of integers\n",
        "data = [char_to_idx[ch] for ch in text]"
      ],
      "metadata": {
        "id": "fWE5h1vNpDMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM Model\n",
        "class LSTMTextGenerator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMTextGenerator, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize hidden and cell states to zeros\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
        "                torch.zeros(1, batch_size, self.hidden_size))"
      ],
      "metadata": {
        "id": "lsagdntspEQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the character indices\n",
        "def one_hot_encode(sequence, n_labels):\n",
        "    one_hot = np.zeros((len(sequence), n_labels), dtype=np.float32)\n",
        "    for i, value in enumerate(sequence):\n",
        "        one_hot[i, value] = 1.0\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "KoCZfuj6pJ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(model, start_str, char_to_idx, idx_to_char, length=100):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Convert the starting string to tensor\n",
        "    input_data = [char_to_idx[ch] for ch in start_str]\n",
        "    input_tensor = torch.tensor(one_hot_encode(input_data, input_size)).unsqueeze(0)\n",
        "\n",
        "    hidden = model.init_hidden(1)\n",
        "    predicted_text = start_str\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_tensor, hidden)\n",
        "        output = output[:, -1, :]  # Get the last prediction\n",
        "        _, top_idx = torch.topk(output, k=1)\n",
        "        predicted_char = idx_to_char[top_idx.item()]\n",
        "        predicted_text += predicted_char\n",
        "\n",
        "        # Prepare next input\n",
        "        input_tensor = torch.tensor(one_hot_encode([top_idx.item()], input_size)).unsqueeze(0)\n",
        "\n",
        "    return predicted_text"
      ],
      "metadata": {
        "id": "E44XFFi0pLdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters and data preparation\n",
        "n_epochs = 100\n",
        "model = LSTMTextGenerator(input_size, hidden_size, input_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ussh2jOFpMw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training (sliding window approach)\n",
        "def get_batches(data, seq_length, batch_size):\n",
        "    n_batches = len(data) // (seq_length * batch_size)\n",
        "    data = data[:n_batches * batch_size * seq_length]\n",
        "    data = np.array(data)\n",
        "    data = data.reshape((batch_size, -1))\n",
        "    for i in range(0, data.shape[1], seq_length):\n",
        "        x = data[:, i:i+seq_length]\n",
        "        y = np.roll(x, shift=-1, axis=1)  # Shift the input sequence to get the target\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "x5FlJpthpO-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Training Loop\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for x_batch, y_batch in get_batches(data, seq_length, batch_size):\n",
        "        x_batch = torch.tensor(one_hot_encode(x_batch.flatten(), input_size)).view(batch_size, seq_length, -1)\n",
        "        y_batch = torch.tensor(y_batch.flatten())\n",
        "\n",
        "        hidden = tuple([h.data for h in hidden])  # Detach hidden states\n",
        "        output, hidden = model(x_batch, hidden)\n",
        "        loss = criterion(output.view(-1, input_size), y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrr1RUEvpQPL",
        "outputId": "272847d4-f9b4-4a7d-aa8c-dd240babb7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 3.3585872650146484\n",
            "Epoch: 10, Loss: 2.8341803550720215\n",
            "Epoch: 20, Loss: 2.422710657119751\n",
            "Epoch: 30, Loss: 2.250765085220337\n",
            "Epoch: 40, Loss: 2.1512398719787598\n",
            "Epoch: 50, Loss: 2.0786097049713135\n",
            "Epoch: 60, Loss: 2.0189108848571777\n",
            "Epoch: 70, Loss: 1.9676964282989502\n",
            "Epoch: 80, Loss: 1.921061396598816\n",
            "Epoch: 90, Loss: 1.8763647079467773\n",
            "CPU times: user 1min 4s, sys: 32 s, total: 1min 35s\n",
            "Wall time: 1min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation\n",
        "start_string = \"Thank you\"  # Provide a starting string\n",
        "generated_text = generate_text(model, start_string, char_to_idx, idx_to_char, length=300)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1YrAECvpRci",
        "outputId": "b6dcf73a-7222-4f1d-9149-cba668a7e1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you have the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the sere the promes and the se\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzaiJZWVpUYz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}