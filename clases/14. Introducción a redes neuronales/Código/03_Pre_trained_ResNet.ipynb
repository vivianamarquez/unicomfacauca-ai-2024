{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task description\n",
        "\n",
        "The task is to use a pre-trained ResNet-18 model as the starting point and adapt it to classify images into one of the 10 classes in the CIFAR-10 dataset. This is a common practice known as transfer learning, where you leverage the knowledge a model has gained from a prior task (in this case, image classification on ImageNet) to improve performance on a new, related task.\n",
        "\n",
        "By using the pre-trained ResNet-18 model, you benefit from the model's already learned filters and weights that can detect edges, textures, and patterns efficiently. Since these low-level features are similar across different image datasets, the pre-trained model only needs to be fine-tuned to learn the specifics of the new dataset, which in this case are the distinct features of the CIFAR-10 classes. **This fine-tuning typically requires less data and computation time than training a model from scratch.**\n",
        "\n",
        "### CIFAR-10\n",
        "In the CIFAR-10 dataset, the 10 classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The dataset consists of 60,000 32x32 color images in total, with 6,000 images per class. The dataset is split into 50,000 training images and 10,000 test images.\n",
        "\n",
        "More info at: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "metadata": {
        "id": "dfqfMj_HcsJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYgOf4FcZjJF"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "1EyOGDn2cPx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a pre-trained model\n",
        "\n",
        "We load a pre-trained ResNet-18 model. ResNet-18 is a convolutional neural network with 18 layers. It's already trained on ImageNet, a large dataset with over a million images and 1000 classes."
      ],
      "metadata": {
        "id": "eQKXv4jtccMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.to(device)  # Move the model to the CUDA device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6FUk6lVaAak",
        "outputId": "aa2a945e-ceee-44ea-ffd2-e27f7a200425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet-18 was chosen as an example because it's a relatively lightweight version of the ResNet family, which makes it faster to train and more suitable for a demo, especially on limited computational resources. ResNet architectures are indeed commonly pre-trained on the ImageNet dataset, which includes a wide range of images across 1000 classes, making the features learned by these models quite robust for a variety of tasks.\n",
        "\n",
        "More info at: https://pytorch.org/vision/main/models/resnet.html"
      ],
      "metadata": {
        "id": "lr8G7oXHaL7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify the Model for a New Task\n",
        "Since we might not be classifying 1000 classes, we change the last fully connected layer to match the number of classes we have. In this example, it's set to 10."
      ],
      "metadata": {
        "id": "DXAjbQA1dcAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the last fully connected layer for the new task\n",
        "num_ftrs = model.fc.in_features # fc=fully connected layer\n",
        "model.fc = nn.Linear(num_ftrs, 10)  # CIFAR-10 has 10 classes\n",
        "model.fc.to(device)  # Move the new fully connected layer to the CUDA device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91nofrmTaHwT",
        "outputId": "45ce8702-0034-482b-a0d5-da64e0a21538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=10, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_ftrs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al3bI9z8dp1O",
        "outputId": "0862b604-9edc-4c57-e004-4a26a4f3a323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value num_ftrs being 512 for ResNet-18 specifically refers to the number of output features from the final convolutional layer of the network."
      ],
      "metadata": {
        "id": "xLDONb6Vd4I2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform the Input Data\n",
        "Input data needs to be pre-processed to the format the pre-trained model expects. This usually includes resizing, cropping, converting to a tensor, and normalizing to match the ImageNet training data."
      ],
      "metadata": {
        "id": "qUiaj2ifeATN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the input data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to size used by ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "w30xq4hzdruy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: This transformation standardizes the pixel values of the input images. The mean and std arguments are lists corresponding to the means and standard deviations for each of the three color channels (RGB) of the ImageNet dataset. By normalizing the images, we ensure that the input distribution matches the distribution that the model was originally trained with. This step is crucial because it helps the model to learn more effectively and converge faster during training.\n",
        "\n",
        "The values [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225] are the global mean and standard deviation of the ImageNet dataset, and using these specific numbers is a common practice when working with models pre-trained on ImageNet.\n",
        "Normalization is done by subtracting the mean from each pixel and then dividing by the standard deviation, channel-wise: (pixel_value - mean) / std."
      ],
      "metadata": {
        "id": "snCCWHwWgVtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "We use ImageFolder to load our dataset, which is assumed to be organized into a directory with subdirectories for each class, each containing the corresponding images."
      ],
      "metadata": {
        "id": "I9b6Rc8Kgi-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9oVUh8JgDa_",
        "outputId": "8bda3fb2-6461-4ae5-9b96-456c6ee8b771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load your own images:\n",
        "\n",
        "```\n",
        "train_data = ImageFolder(root='root_dir/train', transform=transform)\n",
        "val_data = ImageFolder(root='root_dir/val', transform=transform)\n",
        "test_data = ImageFolder(root='root_dir/test', transform=transform)\n",
        "```\n",
        "\n",
        "When using the ImageFolder class in PyTorch, you do not need to specify a train parameter. The ImageFolder class is designed to automatically load images from a directory where the structure of the directory defines the class labels.\n",
        "\n",
        "```\n",
        "root_dir/\n",
        "    train/\n",
        "        class1/\n",
        "            img1.jpg\n",
        "            img2.jpg\n",
        "        class2/\n",
        "            img3.jpg\n",
        "            ...\n",
        "    val/\n",
        "        class1/\n",
        "            img4.jpg\n",
        "            ...\n",
        "        class2/\n",
        "            img5.jpg\n",
        "            ...\n",
        "```"
      ],
      "metadata": {
        "id": "8lOPqfTthSPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data Loaders\n",
        "We create DataLoader instances that provide an iterable over our dataset with the specified batch size and shuffling option for the training data."
      ],
      "metadata": {
        "id": "qUgL850_iAx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "xAn_sJ36glI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Optimizer\n",
        "We define an optimizer that will update the weights of the last fully connected layer. SGD with momentum is used, but the choice of optimizer might vary based on the specific problem."
      ],
      "metadata": {
        "id": "xBqNFBWFimue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss().to(device)  # Move the loss function to the CUDA device"
      ],
      "metadata": {
        "id": "8VVHj7x4imZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "We iterate over the epochs, then over the batches of data in the train_loader, calculate the loss, and update the model's weights."
      ],
      "metadata": {
        "id": "L_rkrv7EiyD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(y_pred, y_true):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "    correct_pred = (y_pred_tags == y_true).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "    acc = torch.round(acc * 100)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "yBE93bbRlepZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the CUDA device\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item() * inputs.size(0) # the total loss over all batches within the epoch\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_running_loss = 0.0\n",
        "    val_running_accuracy = 0.0\n",
        "    with torch.no_grad():  # No need to track the gradients, reducing memory usage and speeding up computations.\n",
        "        for val_inputs, val_labels in val_loader:\n",
        "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "            val_outputs = model(val_inputs) # Foward pass\n",
        "            val_loss = criterion(val_outputs, val_labels) # Compute the loss\n",
        "            val_running_loss += val_loss.item() * val_inputs.size(0) # Cumulative loss\n",
        "            val_running_accuracy += calculate_accuracy(val_outputs, val_labels).item() # Cumulative accuracy\n",
        "\n",
        "    # Calculate average losses\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "    val_epoch_accuracy = val_running_accuracy / len(val_loader)\n",
        "\n",
        "    # Print out the information\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgGYraGiiymW",
        "outputId": "256386eb-4fd9-4ae6-e4e0-62b5c97df5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Training Loss: 0.6638, Validation Loss: 0.6203, Validation Accuracy: 78.62%\n",
            "Epoch 2/5 - Training Loss: 0.6326, Validation Loss: 0.5978, Validation Accuracy: 79.71%\n",
            "Epoch 3/5 - Training Loss: 0.6143, Validation Loss: 0.5778, Validation Accuracy: 80.25%\n",
            "Epoch 4/5 - Training Loss: 0.6011, Validation Loss: 0.5916, Validation Accuracy: 80.12%\n",
            "Epoch 5/5 - Training Loss: 0.5939, Validation Loss: 0.5831, Validation Accuracy: 80.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall performance metrics"
      ],
      "metadata": {
        "id": "WubLG2_Lm4vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming training is complete and the model is already trained...\n",
        "\n",
        "# Evaluate on the test dataset\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_running_accuracy = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "# No need to track the gradients since we are not training\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the CUDA device\n",
        "        outputs = model(inputs)  # Forward pass: compute the predicted outputs\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted classes from the maximum value\n",
        "        total_samples += labels.size(0)  # Increment the total count of samples\n",
        "        test_running_accuracy += (predicted == labels).sum().item()  # Increment the correct predictions count\n",
        "\n",
        "# Calculate the overall accuracy by dividing the number of correct predictions by the total number of samples\n",
        "overall_test_accuracy = (test_running_accuracy / total_samples) * 100\n",
        "\n",
        "# Print overall accuracy\n",
        "print(f'Overall Test Accuracy: {overall_test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOgluI1ni-Jg",
        "outputId": "ab80e0c1-696b-4cc0-a3d5-af491cbfe364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Test Accuracy: 80.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model after training"
      ],
      "metadata": {
        "id": "8-PBzSLzwuHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), 'model_cifar10.pth')"
      ],
      "metadata": {
        "id": "feFFgZMiwtco"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}